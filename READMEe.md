# Product-Titles-Classification-using-SVMs

# Introduction

The research paper titled "Product Title vs. Text Classification" investigates various methods for classifying product titles into predefined categories using SVM (Support Vector Machine) models. The study focuses on enhancing the accuracy of classification through different feature extraction techniques and SVM configurations.

# Objective

The primary objective of this research paper is to develop a robust machine learning model that can accurately classify product titles into specific categories. This involves experimenting with various feature extraction methods and SVM models to determine the optimal approach for this task.

# Data Preprocessing

- **Text Normalization:** The product titles are converted to lowercase to ensure uniformity.
- **Tokenization:** The titles are tokenized into individual words or phrases.
- **Label Encoding:** Categories are encoded as integers using label encoding to convert categorical labels into a numerical format suitable for machine learning algorithms.
- **TF-IDF Vectorization:** The text data is transformed into numerical vectors using TF-IDF, which assigns weights to words based on their frequency and importance within the dataset. The paper specifically compares unigrams (single words) and bigrams (pairs of consecutive words) for this purpose.

# Feature Extraction Techniques

- **Unigrams:** Each word in the product title is considered a separate feature. This is the simplest form of text representation.
- **Bigrams:** Pairs of consecutive words are treated as features. This method captures some context that unigrams might miss.
- **Degree-2 Polynomial Mapping (poly2):** This method expands the feature space by including squared terms and interaction terms of the original features. The resulting feature vector for a title includes the original unigrams, the squares of these unigrams, and the products of each pair of unigrams.

# SVM Models

- **One-vs-One (OvO):** Constructs a binary classifier for every pair of classes. For \(n\), \( \frac{n(n-1)}{2} \) classifiers are trained.
- **One-vs-Rest (OvR):** Constructs a binary classifier for each class against all other classes. For \(n\), \( n \) classifiers are trained.
- **Crammer & Singer:** A multiclass SVM method that solves a single optimization problem for all classes simultaneously, unlike OvO and OvR which decompose the problem into multiple binary classifications.

# Comparison of Methods

The paper evaluates the performance of different SVM models and feature extraction techniques using the Relative Error (REbaseline) metric. The key findings include:

- **Bigram Features:** Incorporating bigrams significantly improves classification performance compared to using unigrams alone.
- **Polynomial Features (poly2):** This method shows the best results by reducing the number of classification errors to around 70% of the baseline error.

## SVM Models

- **One-vs-One (OvO) and One-vs-Rest (OvR):** Both models show competitive performance, but OvO tends to be more computationally intensive due to the larger number of classifiers.
- **Crammer & Singer:** This method generally outperforms the others in terms of both accuracy and computational efficiency. It requires solving a single optimization problem, making it more efficient for large datasets.

# Results

The research shows that advanced feature extraction methods like bigrams and polynomial mappings can greatly enhance the accuracy of product title classification. Specific findings include:

- **Bigram + Unigram:** Using a combination of bigrams and unigrams results in 11,799,345 features.
- **Polynomial Mapping (poly2):** Expanding the feature set using polynomial mappings results in 41,689,205 features. This method provides slightly better results across all three SVM strategies compared to bigram + unigram.

## Performance Metrics

- **REbaseline:** The best results show a reduction in the number of errors to approximately 70% of the baseline error.

# Implementation Details

The paper emphasizes the importance of efficient implementation techniques to handle the large number of features generated by bigram and polynomial mappings. Key implementation details include:

- **Normalization:** Each instance is normalized to have unit length to ensure that no single feature dominates the others.
- **Hashing Technique:** To efficiently manage the vast feature space, a hashing technique is employed to remove unnecessary features that never have non-zero values in the training set.

# Conclusion

The research paper concludes that using sophisticated feature extraction techniques such as bigrams and polynomial mappings can significantly improve the accuracy of product title classification. Among the SVM models, the Crammer & Singer method is recommended for its superior performance and computational efficiency. This study highlights the importance of feature engineering in text classification tasks and provides valuable insights into the implementation of effective SVM models for multi-class classification.
